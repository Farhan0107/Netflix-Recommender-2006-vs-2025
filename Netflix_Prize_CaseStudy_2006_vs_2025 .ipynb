{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9763ba23",
   "metadata": {},
   "source": [
    "\n",
    "# Netflix Prize Case Study – 2006 vs 2025 \n",
    "\n",
    "This notebook is structured so you can **show live implementation** \n",
    "- **2006-style baseline:** SVD++ (classical matrix factorization) using the `surprise` library.\n",
    "- **2025-style model (deep dive):** **HG-TNR (Hybrid Graph + Temporal Transformer Recommender)** – a compact, production-friendly variant implemented in PyTorch that:\n",
    "  - Learns **item-item relations** (graph regularization from co-rating co-occurrence).\n",
    "  - Models **temporal sequences** per user with a **Transformer** (attention over time).\n",
    "  - Predicts **ratings** (for RMSE) and can compute **ranking metrics** (NDCG@K).\n",
    "\n",
    "> **Tip:** If you do not have the Netflix Prize files locally, the notebook will automatically download **MovieLens 100K** as a lightweight, classroom-friendly proxy dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cabc39-2441-40a5-9bdd-6ea5291b7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, numpy, surprise\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"NumPy:\", numpy.__version__)\n",
    "print(\"Surprise:\", surprise.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2639ae",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Setup\n",
    "Run the cell below **once** to install required packages in your environment (skip if already installed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you haven't installed packages in your conda env, do this once:\n",
    "# !pip install numpy==1.26.4 scikit-surprise==1.1.4 pandas matplotlib torch torchvision torchaudio networkx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from surprise import Dataset, SVDpp\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e44c5",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports & Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f184df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the lightweight MovieLens-100K dataset\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# Split into train and test\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e14afd",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data: Netflix Prize (if available) or MovieLens 100K (auto-download)\n",
    "- If you already have Netflix Prize data locally, set `USE_NETFLIX = True` and point to the path.\n",
    "- Otherwise, we will **auto-download MovieLens 100K**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_NETFLIX = False  # Set to True if you have the Netflix Prize data locally\n",
    "NETFLIX_DIR = Path('./netflix-prize-data')  # change this to your local path if you have it\n",
    "\n",
    "DATA_DIR = Path('./data')\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def load_movielens_100k():\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "    zip_path = DATA_DIR / 'ml-100k.zip'\n",
    "    extract_dir = DATA_DIR / 'ml-100k'\n",
    "    if not extract_dir.exists():\n",
    "        import requests\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        with open(zip_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            z.extractall(DATA_DIR)\n",
    "    ratings_path = extract_dir / 'u.data'\n",
    "    movies_path = extract_dir / 'u.item'\n",
    "    # u.data columns: user_id, item_id, rating, timestamp (tab separated)\n",
    "    ratings = pd.read_csv(ratings_path, sep='\\t', names=['userId','movieId','rating','timestamp'], engine='python')\n",
    "    # u.item columns are pipe-separated; include title & year where available\n",
    "    movies = pd.read_csv(movies_path, sep='\\|', header=None, encoding='latin-1')\n",
    "    movies = movies[[0,1,2]]  # movieId, title, release_date\n",
    "    movies.columns = ['movieId','title','release_date']\n",
    "    return ratings, movies\n",
    "\n",
    "def load_netflix_local(netflix_dir: Path):\n",
    "    # Netflix Prize layout has multiple files like 'combined_data_1.txt'...\n",
    "    # Each file contains lines of \"movieId:\" then user ratings rows \"userId,rating,timestamp\"\n",
    "    # We'll parse a subset for demo (first ~1M ratings) to keep it light.\n",
    "    import csv\n",
    "    files = [netflix_dir / f'combined_data_{i}.txt' for i in range(1,5)]\n",
    "    data = []\n",
    "    current_movie = None\n",
    "    for fp in files:\n",
    "        if not fp.exists():\n",
    "            continue\n",
    "        with open(fp, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.endswith(':'):\n",
    "                    current_movie = int(line[:-1])\n",
    "                else:\n",
    "                    if not current_movie:\n",
    "                        continue\n",
    "                    parts = line.split(',')\n",
    "                    if len(parts) == 3:\n",
    "                        uid, r, ts = parts\n",
    "                        data.append((int(uid), int(current_movie), int(r), int(ts)))\n",
    "                if len(data) >= 1_000_000:  # cap for demo speed\n",
    "                    break\n",
    "        if len(data) >= 1_000_000:\n",
    "            break\n",
    "    ratings = pd.DataFrame(data, columns=['userId','movieId','rating','timestamp'])\n",
    "    movies = pd.DataFrame(columns=['movieId','title','release_date'])  # titles not included in prize files\n",
    "    return ratings, movies\n",
    "\n",
    "if USE_NETFLIX and NETFLIX_DIR.exists():\n",
    "    print('Loading Netflix Prize subset from', NETFLIX_DIR.resolve())\n",
    "    ratings_df, movies_df = load_netflix_local(NETFLIX_DIR)\n",
    "else:\n",
    "    print('Downloading MovieLens 100K (first time only) ...')\n",
    "    ratings_df, movies_df = load_movielens_100k()\n",
    "\n",
    "print('Ratings shape:', ratings_df.shape)\n",
    "print(ratings_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3b289",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Preprocessing\n",
    "- Map `userId` and `movieId` to contiguous indices.\n",
    "- Sort interactions per user by `timestamp` (needed for temporal modeling).\n",
    "- Train/Validation/Test split by **time per user** (last interactions to test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map IDs to contiguous indices\n",
    "uid_map = {u:i for i,u in enumerate(ratings_df['userId'].unique())}\n",
    "iid_map = {i:j for j,i in enumerate(ratings_df['movieId'].unique())}\n",
    "\n",
    "ratings_df['uid'] = ratings_df['userId'].map(uid_map)\n",
    "ratings_df['iid'] = ratings_df['movieId'].map(iid_map)\n",
    "\n",
    "# Normalize timestamps to seconds (ML-100K is already epoch seconds)\n",
    "if 'timestamp' not in ratings_df.columns:\n",
    "    ratings_df['timestamp'] = 0\n",
    "\n",
    "# Sort by user then time\n",
    "ratings_df = ratings_df.sort_values(['uid','timestamp']).reset_index(drop=True)\n",
    "\n",
    "n_users = ratings_df['uid'].nunique()\n",
    "n_items = ratings_df['iid'].nunique()\n",
    "print(f'Users: {n_users}, Items: {n_items}, Interactions: {len(ratings_df)}')\n",
    "\n",
    "# Time-based split per user: last 2 interactions -> test, previous 2 -> val (if available)\n",
    "def train_val_test_split_per_user(df):\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for u, g in df.groupby('uid'):\n",
    "        idxs = g.index.tolist()\n",
    "        if len(idxs) <= 4:\n",
    "            # small histories: 1 test, 1 val if possible\n",
    "            if len(idxs) >= 1: test_idx.append(idxs[-1])\n",
    "            if len(idxs) >= 2: val_idx.append(idxs[-2])\n",
    "            train_idx += idxs[:-2] if len(idxs) > 2 else []\n",
    "        else:\n",
    "            test_idx += idxs[-2:]\n",
    "            val_idx += idxs[-4:-2]\n",
    "            train_idx += idxs[:-4]\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "train_idx, val_idx, test_idx = train_val_test_split_per_user(ratings_df)\n",
    "\n",
    "train_df = ratings_df.loc[train_idx].reset_index(drop=True)\n",
    "val_df   = ratings_df.loc[val_idx].reset_index(drop=True)\n",
    "test_df  = ratings_df.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print('Split sizes:', len(train_df), len(val_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a28843",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 2006-Style Baseline: SVD++ (Matrix Factorization with Implicit Feedback)\n",
    "\n",
    "This baseline mirrors the Netflix Prize era approach (matrix factorization with biases and implicit feedback).\n",
    "We evaluate on our **time-based** validation/test splits for fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ae289",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Surprise expects columns: user, item, rating (strings/ids), so we pass original IDs as strings.\n",
    "reader = Reader(rating_scale=(1,5))\n",
    "\n",
    "# Build trainset with our train_df\n",
    "train_surprise = Dataset.load_from_df(train_df[['userId','movieId','rating']].astype(str), reader).build_full_trainset()\n",
    "\n",
    "# Build val/test \"anti\" sets to compute predictions for their pairs\n",
    "val_pairs = list(zip(val_df['userId'].astype(str), val_df['movieId'].astype(str), val_df['rating'].astype(float)))\n",
    "test_pairs = list(zip(test_df['userId'].astype(str), test_df['movieId'].astype(str), test_df['rating'].astype(float)))\n",
    "\n",
    "algo = SVDpp(n_factors=50, n_epochs=20, reg_all=0.02, lr_all=0.005, random_state=42)\n",
    "algo.fit(train_surprise)\n",
    "\n",
    "def eval_pairs(algo, pairs):\n",
    "    y_true, y_pred = [], []\n",
    "    for u,i,r in pairs:\n",
    "        y_true.append(r)\n",
    "        y_pred.append(algo.predict(u,i).est)\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "svdpp_val_rmse = eval_pairs(algo, val_pairs)\n",
    "svdpp_test_rmse = eval_pairs(algo, test_pairs)\n",
    "\n",
    "print(f\"SVD++  Val RMSE: {svdpp_val_rmse:.4f}\")\n",
    "print(f\"SVD++ Test RMSE: {svdpp_test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7ab1f",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Item-Item Graph (Co-occurrence) for Graph Regularization\n",
    "We compute a lightweight **item-item similarity** from co-ratings and keep **top-K neighbors** per item.  \n",
    "This provides a graph-regularization term encouraging similar items to have similar embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f734f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "K_NEIGHBORS = 20  # top-K neighbors per item\n",
    "\n",
    "# Build item -> set(users) map from *train* only\n",
    "item_users = defaultdict(set)\n",
    "for row in train_df[['uid','iid']].itertuples(index=False):\n",
    "    item_users[row.iid].add(row.uid)\n",
    "\n",
    "# Compute Jaccard similarities (sparse, only for items that share users)\n",
    "neighbors = {}\n",
    "items = list(item_users.keys())\n",
    "for i in tqdm(items, desc='Compute item neighbors'):\n",
    "    sims = []\n",
    "    A = item_users[i]\n",
    "    if not A:\n",
    "        neighbors[i] = []\n",
    "        continue\n",
    "    # Sample to limit runtime on larger sets\n",
    "    for j in items:\n",
    "        if j == i: \n",
    "            continue\n",
    "        B = item_users[j]\n",
    "        if not B:\n",
    "            continue\n",
    "        inter = len(A & B)\n",
    "        if inter == 0:\n",
    "            continue\n",
    "        union = len(A | B)\n",
    "        sim = inter / union\n",
    "        if sim > 0:\n",
    "            sims.append((j, sim))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    neighbors[i] = sims[:K_NEIGHBORS]\n",
    "\n",
    "sum_edges = sum(len(v) for v in neighbors.values())\n",
    "print(f\"Total neighbor edges (directed): {sum_edges}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc1d82",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Build Temporal Sequences per User\n",
    "We will create **user sequences** (ordered by time) and sample (sequence, target item, rating) tuples for training the **Transformer** model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQ_LEN = 20  # keep recent history\n",
    "RATING_MEAN = train_df['rating'].mean()\n",
    "\n",
    "def make_user_histories(df):\n",
    "    histories = {}\n",
    "    for uid, g in df.groupby('uid'):\n",
    "        seq = list(zip(g['iid'].tolist(), g['rating'].astype(float).tolist(), g['timestamp'].tolist()))\n",
    "        histories[uid] = seq\n",
    "    return histories\n",
    "\n",
    "train_hist = make_user_histories(train_df)\n",
    "val_hist   = make_user_histories(val_df)\n",
    "test_hist  = make_user_histories(test_df)\n",
    "\n",
    "class SeqRatingDataset(TorchDataset):\n",
    "    def __init__(self, histories, max_len=20):\n",
    "        self.samples = []\n",
    "        for uid, seq in histories.items():\n",
    "            if len(seq) < 2:\n",
    "                continue\n",
    "            for t in range(1, len(seq)):\n",
    "                past = seq[max(0, t-max_len):t]\n",
    "                target_iid, target_rating, target_ts = seq[t]\n",
    "                past_iids = [p[0] for p in past]\n",
    "                past_ts   = [p[2] for p in past]\n",
    "                self.samples.append((uid, past_iids, past_ts, target_iid, target_rating))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # pad sequences with -1\n",
    "    uids, seq_iids, seq_ts, target_iids, target_r = [], [], [], [], []\n",
    "    max_len = max(len(x[1]) for x in batch) if batch else 0\n",
    "    for (uid, past_iids, past_ts, tgt_iid, tgt_r) in batch:\n",
    "        pad_len = max_len - len(past_iids)\n",
    "        uids.append(uid)\n",
    "        seq_iids.append(past_iids + [-1]*pad_len)\n",
    "        seq_ts.append(past_ts + [0]*pad_len)\n",
    "        target_iids.append(tgt_iid)\n",
    "        target_r.append(tgt_r)\n",
    "    return (\n",
    "        torch.tensor(uids, dtype=torch.long),\n",
    "        torch.tensor(seq_iids, dtype=torch.long),\n",
    "        torch.tensor(seq_ts, dtype=torch.long),\n",
    "        torch.tensor(target_iids, dtype=torch.long),\n",
    "        torch.tensor(target_r, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "train_ds = SeqRatingDataset(train_hist, MAX_SEQ_LEN)\n",
    "val_ds   = SeqRatingDataset(val_hist, MAX_SEQ_LEN)\n",
    "test_ds  = SeqRatingDataset(test_hist, MAX_SEQ_LEN)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl   = DataLoader(val_ds, batch_size=512, shuffle=False, collate_fn=collate_fn)\n",
    "test_dl  = DataLoader(test_ds, batch_size=512, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b922fff",
   "metadata": {},
   "source": [
    "\n",
    "## 7. 2025-Style Model: **HG-TNR (Hybrid Graph + Temporal Transformer) – Lite**\n",
    "\n",
    "**Components:**\n",
    "- **Embeddings:** `user_emb`, `item_emb`, plus bias terms.\n",
    "- **Temporal Encoder:** Transformer encoder over the user's historical item sequence.\n",
    "- **Graph Regularization:** Encourage similar items (based on co-occurrence) to have similar embeddings.\n",
    "- **Predictor:** Combines **contextual user state** with target item embedding to **predict rating** (1–5).\n",
    "\n",
    "> This is intentionally compact and runs on CPU in class. You can scale dimensions/epochs for better scores later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fec4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :]\n",
    "\n",
    "class HGTNRLite(nn.Module):\n",
    "    def __init__(self, n_users, n_items, d_model=64, n_heads=4, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.user_emb = nn.Embedding(n_users, d_model)\n",
    "        self.item_emb = nn.Embedding(n_items, d_model)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.posenc = PositionalEncoding(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Prediction head\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(2*d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1)\n",
    "        )\n",
    "\n",
    "        # init\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight)\n",
    "\n",
    "    def forward(self, uid, seq_iids, target_iid):\n",
    "        # Embeddings\n",
    "        seq_mask = (seq_iids >= 0).float()   # 1 for valid, 0 for pad\n",
    "        seq_iids_clamped = seq_iids.clamp(min=0)\n",
    "        seq_e = self.item_emb(seq_iids_clamped) * seq_mask.unsqueeze(-1)\n",
    "\n",
    "        seq_e = self.posenc(seq_e)\n",
    "        # Generate key padding mask: True for PADs\n",
    "        key_padding_mask = (seq_iids < 0)\n",
    "        h = self.encoder(seq_e, src_key_padding_mask=key_padding_mask)  # (B,T,D)\n",
    "\n",
    "        # Take pooled representation (mean over valid positions)\n",
    "        denom = seq_mask.sum(dim=1, keepdim=True).clamp(min=1.0)\n",
    "        user_ctx = (h * seq_mask.unsqueeze(-1)).sum(dim=1) / denom\n",
    "\n",
    "        # Combine with static user embedding (gated)\n",
    "        u_static = self.user_emb(uid)\n",
    "        user_repr = self.dropout(torch.cat([user_ctx, u_static], dim=-1))  # (B, 2D)\n",
    "\n",
    "        item_repr = self.item_emb(target_iid)\n",
    "        x = torch.cat([user_ctx, item_repr], dim=-1)  # contextual user + target item\n",
    "\n",
    "        pred = self.out(x).squeeze(-1)\n",
    "        # Add biases and clamp to 1..5 during eval (not during training to allow gradient flow)\n",
    "        pred = pred + self.item_bias(target_iid).squeeze(-1) + self.user_bias(uid).squeeze(-1)\n",
    "        return pred\n",
    "\n",
    "def graph_reg_loss(item_emb_table, neighbors, lam=1e-4, device='cpu'):\n",
    "    # Sum lam * w_ij * ||e_i - e_j||^2 over neighbor pairs\n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "    for i, neighs in neighbors.items():\n",
    "        if not neighs:\n",
    "            continue\n",
    "        e_i = item_emb_table.weight[i]\n",
    "        for j, w in neighs:\n",
    "            e_j = item_emb_table.weight[j]\n",
    "            loss = loss + lam * w * torch.sum((e_i - e_j) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a64ec",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Train the HG-TNR Model\n",
    "We optimize **MSE (for ratings)** + **Graph Regularization**. We report **RMSE** on validation/test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96860603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model = HGTNRLite(n_users=n_users, n_items=n_items, d_model=64, n_heads=4, n_layers=2, dropout=0.1).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "EPOCHS = 5  # keep small for classroom demo; increase later (e.g., 15-30) for better results\n",
    "LAMBDA_GRAPH = 1e-5\n",
    "\n",
    "def run_epoch(dl, train=True):\n",
    "    model.train(train)\n",
    "    losses = []\n",
    "    for (uids, seq_iids, seq_ts, tgt_iids, tgt_r) in dl:\n",
    "        uids = uids.to(device)\n",
    "        seq_iids = seq_iids.to(device)\n",
    "        tgt_iids = tgt_iids.to(device)\n",
    "        tgt_r = tgt_r.to(device)\n",
    "\n",
    "        preds = model(uids, seq_iids, tgt_iids)\n",
    "        mse = F.mse_loss(preds, tgt_r)\n",
    "\n",
    "        loss = mse\n",
    "        if train and LAMBDA_GRAPH > 0:\n",
    "            loss = loss + graph_reg_loss(model.item_emb, neighbors, lam=LAMBDA_GRAPH, device=device)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    return float(np.mean(losses))\n",
    "\n",
    "def evaluate_rmse(dl):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for (uids, seq_iids, seq_ts, tgt_iids, tgt_r) in dl:\n",
    "            uids = uids.to(device)\n",
    "            seq_iids = seq_iids.to(device)\n",
    "            tgt_iids = tgt_iids.to(device)\n",
    "            preds = model(uids, seq_iids, tgt_iids)\n",
    "            # Clamp predictions to rating range for RMSE eval\n",
    "            preds = preds.clamp(1.0, 5.0)\n",
    "            y_true.append(tgt_r.numpy())\n",
    "            y_pred.append(preds.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "history = {'train_loss':[], 'val_rmse':[]}\n",
    "best_val = 1e9\n",
    "best_state = None\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tl = run_epoch(train_dl, train=True)\n",
    "    vr = evaluate_rmse(val_dl)\n",
    "    history['train_loss'].append(tl)\n",
    "    history['val_rmse'].append(vr)\n",
    "    print(f'Epoch {ep:02d}: train_loss={tl:.4f}  val_RMSE={vr:.4f}')\n",
    "    if vr < best_val:\n",
    "        best_val = vr\n",
    "        best_state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "\n",
    "# Load best and evaluate on test\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "test_rmse = evaluate_rmse(test_dl)\n",
    "print(f\"HG-TNR Test RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d434e49",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Compare Baseline vs HG-TNR & Plot Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4cdb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"SVD++ Test RMSE: {svdpp_test_rmse:.4f}\")\n",
    "print(f\"HG-TNR Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_rmse'], label='Val RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.title('HG-TNR Training')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febdc01c",
   "metadata": {},
   "source": [
    "\n",
    "## 10. (Optional) Simple Ranking Metric @10\n",
    "For a small random user set, compute **Hit@10** by ranking items by predicted rating using the user's context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d8b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hit_at_k(model, user_ids, k=10, sample_items=500):\n",
    "    model.eval()\n",
    "    hits = []\n",
    "    rng = np.random.default_rng(42)\n",
    "    with torch.no_grad():\n",
    "        for u in user_ids:\n",
    "            hist = train_hist.get(u, [])\n",
    "            if len(hist) < 1:\n",
    "                continue\n",
    "            # Build a recent sequence\n",
    "            seq = hist[-MAX_SEQ_LEN:]\n",
    "            seq_iids = [x[0] for x in seq]\n",
    "            # Pick a target from val/test for this user if available\n",
    "            target_pool = (val_hist.get(u, []) + test_hist.get(u, []))\n",
    "            if not target_pool:\n",
    "                continue\n",
    "            tgt_iid = target_pool[-1][0]\n",
    "\n",
    "            # Negative sampling (random items)\n",
    "            cand = set([tgt_iid])\n",
    "            while len(cand) < sample_items:\n",
    "                cand.add(rng.integers(0, n_items))\n",
    "            cand = list(cand)\n",
    "\n",
    "            # Score all candidates\n",
    "            uid_t = torch.tensor([u]*len(cand)).to(device)\n",
    "            seq_pad = torch.tensor([seq_iids + [-1]*(MAX_SEQ_LEN-len(seq_iids))]*len(cand)).to(device)\n",
    "            cand_t = torch.tensor(cand).to(device)\n",
    "            scores = model(uid_t, seq_pad, cand_t).cpu().numpy()\n",
    "\n",
    "            # Rank and check hit\n",
    "            topk = np.argsort(-scores)[:k]\n",
    "            top_items = [cand[i] for i in topk]\n",
    "            hits.append(1.0 if tgt_iid in top_items else 0.0)\n",
    "    if hits:\n",
    "        return float(np.mean(hits))\n",
    "    return float('nan')\n",
    "\n",
    "sample_users = list(train_hist.keys())[:200]\n",
    "h10 = hit_at_k(model, sample_users, k=10, sample_items=500)\n",
    "print(f\"HG-TNR Hit@10 (sample users): {h10:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36503ca6",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Save Artifacts (Optional)\n",
    "Save the trained HG-TNR model state dict and metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aaef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ART_DIR = Path('./artifacts')\n",
    "ART_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "torch.save(model.state_dict(), ART_DIR / 'hgtnr_lite_state_dict.pt')\n",
    "with open(ART_DIR / 'metrics.json','w') as f:\n",
    "    json.dump({\n",
    "        'svdpp_val_rmse': svdpp_val_rmse,\n",
    "        'svdpp_test_rmse': svdpp_test_rmse,\n",
    "        'hgtnr_test_rmse': test_rmse\n",
    "    }, f, indent=2)\n",
    "\n",
    "print('Saved artifacts to', ART_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08887f14",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### What to demonstrate live\n",
    "1. Show the **baseline SVD++** cell and its RMSE (connect to 2006 Netflix Prize era).\n",
    "2. Explain the **graph neighbors** computation (why it helps long-tail similarity).\n",
    "3. Walk through the **HG-TNR model** (Transformer for temporal context + graph reg for item geometry).\n",
    "4. Train for a few epochs (already set short) and show **val/test RMSE** and the training plot.\n",
    "5. If time permits, run **Hit@10** for a ranking flavor.\n",
    "\n",
    "> You can later scale embedding sizes, epochs, and graph K to approach better accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (netflix_env)",
   "language": "python",
   "name": "netflix_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
